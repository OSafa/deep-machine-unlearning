{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-06-19T21:52:45.562708Z","iopub.status.busy":"2024-06-19T21:52:45.562430Z","iopub.status.idle":"2024-06-19T21:53:03.209036Z","shell.execute_reply":"2024-06-19T21:53:03.208252Z","shell.execute_reply.started":"2024-06-19T21:52:45.562683Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["import os\n","import sys\n","import argparse\n","import re\n","import time\n","import random\n","from datetime import datetime\n","from typing import Any, Tuple, Dict, List\n","from copy import deepcopy\n","import copy\n","import math\n","import shutil\n","\n","\n","from tqdm import tqdm\n","from sklearn import linear_model, model_selection\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression\n","import numpy as np\n","import torch\n","from torch.optim.lr_scheduler import _LRScheduler\n","from torch.nn import functional as F\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","from torch.utils.data import DataLoader, Dataset, Subset, ConcatDataset, dataset\n","import torchvision.transforms as transforms\n","from torchvision.datasets import CIFAR100, CIFAR10, ImageFolder\n","from torchvision.models import resnet18\n","from torch.autograd import Variable\n","from torchvision import datasets, models, transforms\n","import matplotlib.pyplot as plt\n","from transformers import ViTModel, ViTFeatureExtractor\n","import seaborn as sns\n","import scipy.stats as stats\n","import pandas as pd\n","from transformers import AutoTokenizer, AutoModel, Trainer, TrainingArguments\n","from sklearn.model_selection import train_test_split\n","from transformers.data.processors import SingleSentenceClassificationProcessor, InputFeatures\n","from transformers import AutoModel, AutoTokenizer , AutoModelForSequenceClassification, AutoConfig"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-06-19T21:53:03.218645Z","iopub.status.busy":"2024-06-19T21:53:03.218320Z","iopub.status.idle":"2024-06-19T21:53:03.249175Z","shell.execute_reply":"2024-06-19T21:53:03.248311Z","shell.execute_reply.started":"2024-06-19T21:53:03.218615Z"},"trusted":true},"outputs":[],"source":["class BasicBlock(nn.Module):\n","    \"\"\"Basic Block for resnet 18 and resnet 34\"\"\"\n","\n","    # BasicBlock and BottleNeck block\n","    # have different output size\n","    # we use class attribute expansion\n","    # to distinct\n","    expansion = 1\n","\n","    def __init__(self, in_channels, out_channels, stride=1):\n","        super().__init__()\n","\n","        # residual function\n","        self.residual_function = nn.Sequential(\n","            nn.Conv2d(\n","                in_channels,\n","                out_channels,\n","                kernel_size=3,\n","                stride=stride,\n","                padding=1,\n","                bias=False,\n","            ),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(\n","                out_channels,\n","                out_channels * BasicBlock.expansion,\n","                kernel_size=3,\n","                padding=1,\n","                bias=False,\n","            ),\n","            nn.BatchNorm2d(out_channels * BasicBlock.expansion),\n","        )\n","\n","        # shortcut\n","        self.shortcut = nn.Sequential()\n","\n","        # the shortcut output dimension is not the same with residual function\n","        # use 1*1 convolution to match the dimension\n","        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(\n","                    in_channels,\n","                    out_channels * BasicBlock.expansion,\n","                    kernel_size=1,\n","                    stride=stride,\n","                    bias=False,\n","                ),\n","                nn.BatchNorm2d(out_channels * BasicBlock.expansion),\n","            )\n","\n","    def forward(self, x):\n","        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n","\n","\n","class BottleNeck(nn.Module):\n","    \"\"\"Residual block for resnet over 50 layers\"\"\"\n","\n","    expansion = 4\n","\n","    def __init__(self, in_channels, out_channels, stride=1):\n","        super().__init__()\n","        self.residual_function = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(\n","                out_channels,\n","                out_channels,\n","                stride=stride,\n","                kernel_size=3,\n","                padding=1,\n","                bias=False,\n","            ),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(\n","                out_channels,\n","                out_channels * BottleNeck.expansion,\n","                kernel_size=1,\n","                bias=False,\n","            ),\n","            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n","        )\n","\n","        self.shortcut = nn.Sequential()\n","\n","        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(\n","                    in_channels,\n","                    out_channels * BottleNeck.expansion,\n","                    stride=stride,\n","                    kernel_size=1,\n","                    bias=False,\n","                ),\n","                nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n","            )\n","\n","    def forward(self, x):\n","        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_block, num_classes=100):\n","        super().__init__()\n","\n","        self.in_channels = 64\n","\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(inplace=True),\n","        )\n","        # we use a different inputsize than the original paper\n","        # so conv2_x's stride is 1\n","        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\n","        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\n","        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)\n","        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n","        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.fc = nn.Linear(512 * block.expansion, num_classes)\n","\n","    def _make_layer(self, block, out_channels, num_blocks, stride):\n","        \"\"\"make resnet layers(by layer i didnt mean this 'layer' was the\n","        same as a neuron netowork layer, ex. conv layer), one layer may\n","        contain more than one residual block\n","\n","        Args:\n","            block: block type, basic block or bottle neck block\n","            out_channels: output depth channel number of this layer\n","            num_blocks: how many blocks per layer\n","            stride: the stride of the first block of this layer\n","\n","        Return:\n","            return a resnet layer\n","        \"\"\"\n","\n","        # we have num_block blocks per layer, the first block\n","        # could be 1 or 2, other blocks would always be 1\n","        strides = [stride] + [1] * (num_blocks - 1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_channels, out_channels, stride))\n","            self.in_channels = out_channels * block.expansion\n","\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        output = self.conv1(x)\n","        output = self.conv2_x(output)\n","        output = self.conv3_x(output)\n","        output = self.conv4_x(output)\n","        output = self.conv5_x(output)\n","        output = self.avg_pool(output)\n","        output = output.view(output.size(0), -1)\n","        output = self.fc(output)\n","\n","        return output\n","\n","\n","def resnet18():\n","    \"\"\"return a ResNet 18 object\"\"\"\n","    return ResNet(BasicBlock, [2, 2, 2, 2])\n","\n","\n","def resnet34():\n","    \"\"\"return a ResNet 34 object\"\"\"\n","    return ResNet(BasicBlock, [3, 4, 6, 3])\n","\n","\n","def resnet50():\n","    \"\"\"return a ResNet 50 object\"\"\"\n","    return ResNet(BottleNeck, [3, 4, 6, 3])\n","\n","\n","def resnet101():\n","    \"\"\"return a ResNet 101 object\"\"\"\n","    return ResNet(BottleNeck, [3, 4, 23, 3])\n","\n","\n","def resnet152():\n","    \"\"\"return a ResNet 152 object\"\"\"\n","    return ResNet(BottleNeck, [3, 8, 36, 3])"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-06-19T21:53:03.252227Z","iopub.status.busy":"2024-06-19T21:53:03.251907Z","iopub.status.idle":"2024-06-19T21:53:03.293868Z","shell.execute_reply":"2024-06-19T21:53:03.293141Z","shell.execute_reply.started":"2024-06-19T21:53:03.252198Z"},"trusted":true},"outputs":[],"source":["\"\"\" helper function\n","\n","author baiyu\n","\"\"\"\n","# https://github.com/weiaicunzai/pytorch-cifar100\n","\n","def get_network(args):\n","    \"\"\"return given network\"\"\"\n","\n","    if args.net == \"vgg16\":\n","        from models.vgg import vgg16_bn\n","\n","        net = vgg16_bn()\n","    elif args.net == \"vgg13\":\n","        from models.vgg import vgg13_bn\n","\n","        net = vgg13_bn()\n","    elif args.net == \"vgg11\":\n","        from models.vgg import vgg11_bn\n","\n","        net = vgg11_bn()\n","    elif args.net == \"vgg19\":\n","        from models.vgg import vgg19_bn\n","\n","        net = vgg19_bn()\n","    elif args.net == \"densenet121\":\n","        from models.densenet import densenet121\n","\n","        net = densenet121()\n","    elif args.net == \"densenet161\":\n","        from models.densenet import densenet161\n","\n","        net = densenet161()\n","    elif args.net == \"densenet169\":\n","        from models.densenet import densenet169\n","\n","        net = densenet169()\n","    elif args.net == \"densenet201\":\n","        from models.densenet import densenet201\n","\n","        net = densenet201()\n","    elif args.net == \"googlenet\":\n","        from models.googlenet import googlenet\n","\n","        net = googlenet()\n","    elif args.net == \"inceptionv3\":\n","        from models.inceptionv3 import inceptionv3\n","\n","        net = inceptionv3()\n","    elif args.net == \"inceptionv4\":\n","        from models.inceptionv4 import inceptionv4\n","\n","        net = inceptionv4()\n","    elif args.net == \"inceptionresnetv2\":\n","        from models.inceptionv4 import inception_resnet_v2\n","\n","        net = inception_resnet_v2()\n","    elif args.net == \"xception\":\n","        from models.xception import xception\n","\n","        net = xception()\n","    elif args.net == \"resnet18\":\n","        from models.resnet import resnet18\n","\n","        net = resnet18()\n","    elif args.net == \"resnet34\":\n","        from models.resnet import resnet34\n","\n","        net = resnet34()\n","    elif args.net == \"resnet50\":\n","        from models.resnet import resnet50\n","\n","        net = resnet50()\n","    elif args.net == \"resnet101\":\n","        from models.resnet import resnet101\n","\n","        net = resnet101()\n","    elif args.net == \"resnet152\":\n","        from models.resnet import resnet152\n","\n","        net = resnet152()\n","    elif args.net == \"preactresnet18\":\n","        from models.preactresnet import preactresnet18\n","\n","        net = preactresnet18()\n","    elif args.net == \"preactresnet34\":\n","        from models.preactresnet import preactresnet34\n","\n","        net = preactresnet34()\n","    elif args.net == \"preactresnet50\":\n","        from models.preactresnet import preactresnet50\n","\n","        net = preactresnet50()\n","    elif args.net == \"preactresnet101\":\n","        from models.preactresnet import preactresnet101\n","\n","        net = preactresnet101()\n","    elif args.net == \"preactresnet152\":\n","        from models.preactresnet import preactresnet152\n","\n","        net = preactresnet152()\n","    elif args.net == \"resnext50\":\n","        from models.resnext import resnext50\n","\n","        net = resnext50()\n","    elif args.net == \"resnext101\":\n","        from models.resnext import resnext101\n","\n","        net = resnext101()\n","    elif args.net == \"resnext152\":\n","        from models.resnext import resnext152\n","\n","        net = resnext152()\n","    elif args.net == \"shufflenet\":\n","        from models.shufflenet import shufflenet\n","\n","        net = shufflenet()\n","    elif args.net == \"shufflenetv2\":\n","        from models.shufflenetv2 import shufflenetv2\n","\n","        net = shufflenetv2()\n","    elif args.net == \"squeezenet\":\n","        from models.squeezenet import squeezenet\n","\n","        net = squeezenet()\n","    elif args.net == \"mobilenet\":\n","        from models.mobilenet import mobilenet\n","\n","        net = mobilenet()\n","    elif args.net == \"mobilenetv2\":\n","        from models.mobilenetv2 import mobilenetv2\n","\n","        net = mobilenetv2()\n","    elif args.net == \"nasnet\":\n","        from models.nasnet import nasnet\n","\n","        net = nasnet()\n","    elif args.net == \"attention56\":\n","        from models.attention import attention56\n","\n","        net = attention56()\n","    elif args.net == \"attention92\":\n","        from models.attention import attention92\n","\n","        net = attention92()\n","    elif args.net == \"seresnet18\":\n","        from models.senet import seresnet18\n","\n","        net = seresnet18()\n","    elif args.net == \"seresnet34\":\n","        from models.senet import seresnet34\n","\n","        net = seresnet34()\n","    elif args.net == \"seresnet50\":\n","        from models.senet import seresnet50\n","\n","        net = seresnet50()\n","    elif args.net == \"seresnet101\":\n","        from models.senet import seresnet101\n","\n","        net = seresnet101()\n","    elif args.net == \"seresnet152\":\n","        from models.senet import seresnet152\n","\n","        net = seresnet152()\n","    elif args.net == \"wideresnet\":\n","        from models.wideresidual import wideresnet\n","\n","        net = wideresnet()\n","    elif args.net == \"stochasticdepth18\":\n","        from models.stochasticdepth import stochastic_depth_resnet18\n","\n","        net = stochastic_depth_resnet18()\n","    elif args.net == \"stochasticdepth34\":\n","        from models.stochasticdepth import stochastic_depth_resnet34\n","\n","        net = stochastic_depth_resnet34()\n","    elif args.net == \"stochasticdepth50\":\n","        from models.stochasticdepth import stochastic_depth_resnet50\n","\n","        net = stochastic_depth_resnet50()\n","    elif args.net == \"stochasticdepth101\":\n","        from models.stochasticdepth import stochastic_depth_resnet101\n","\n","        net = stochastic_depth_resnet101()\n","\n","    else:\n","        print(\"the network name you have entered is not supported yet\")\n","        sys.exit()\n","\n","    if args.gpu:  # use_gpu\n","        net = net.cuda()\n","\n","    return net\n","\n","\n","def get_training_dataloader(mean, std, batch_size=16, num_workers=2, shuffle=True):\n","    \"\"\"return training dataloader\n","    Args:\n","        mean: mean of cifar100 training dataset\n","        std: std of cifar100 training dataset\n","        path: path to cifar100 training python dataset\n","        batch_size: dataloader batchsize\n","        num_workers: dataloader num_works\n","        shuffle: whether to shuffle\n","    Returns: train_data_loader:torch dataloader object\n","    \"\"\"\n","\n","    transform_train = transforms.Compose(\n","        [\n","            # transforms.ToPILImage(),\n","            transforms.RandomCrop(32, padding=4),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.RandomRotation(15),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean, std),\n","        ]\n","    )\n","    # cifar100_training = CIFAR100Train(path, transform=transform_train)\n","    cifar100_training = torchvision.datasets.CIFAR100(\n","        root=\"./data\", train=True, download=True, transform=transform_train\n","    )\n","    cifar100_training_loader = DataLoader(\n","        cifar100_training,\n","        shuffle=shuffle,\n","        num_workers=num_workers,\n","        batch_size=batch_size,\n","    )\n","\n","    return cifar100_training_loader\n","\n","\n","def get_test_dataloader(mean, std, batch_size=16, num_workers=2, shuffle=True):\n","    \"\"\"return training dataloader\n","    Args:\n","        mean: mean of cifar100 test dataset\n","        std: std of cifar100 test dataset\n","        path: path to cifar100 test python dataset\n","        batch_size: dataloader batchsize\n","        num_workers: dataloader num_works\n","        shuffle: whether to shuffle\n","    Returns: cifar100_test_loader:torch dataloader object\n","    \"\"\"\n","\n","    transform_test = transforms.Compose(\n","        [transforms.ToTensor(), transforms.Normalize(mean, std)]\n","    )\n","    # cifar100_test = CIFAR100Test(path, transform=transform_test)\n","    cifar100_test = torchvision.datasets.CIFAR100(\n","        root=\"./data\", train=False, download=True, transform=transform_test\n","    )\n","    cifar100_test_loader = DataLoader(\n","        cifar100_test, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size\n","    )\n","\n","    return cifar100_test_loader\n","\n","\n","def compute_mean_std(cifar100_dataset):\n","    \"\"\"compute the mean and std of cifar100 dataset\n","    Args:\n","        cifar100_training_dataset or cifar100_test_dataset\n","        witch derived from class torch.utils.data\n","\n","    Returns:\n","        a tuple contains mean, std value of entire dataset\n","    \"\"\"\n","\n","    data_r = numpy.dstack(\n","        [cifar100_dataset[i][1][:, :, 0] for i in range(len(cifar100_dataset))]\n","    )\n","    data_g = numpy.dstack(\n","        [cifar100_dataset[i][1][:, :, 1] for i in range(len(cifar100_dataset))]\n","    )\n","    data_b = numpy.dstack(\n","        [cifar100_dataset[i][1][:, :, 2] for i in range(len(cifar100_dataset))]\n","    )\n","    mean = numpy.mean(data_r), numpy.mean(data_g), numpy.mean(data_b)\n","    std = numpy.std(data_r), numpy.std(data_g), numpy.std(data_b)\n","\n","    return mean, std\n","\n","\n","class WarmUpLR(_LRScheduler):\n","    \"\"\"warmup_training learning rate scheduler\n","    Args:\n","        optimizer: optimzier(e.g. SGD)\n","        total_iters: totoal_iters of warmup phase\n","    \"\"\"\n","\n","    def __init__(self, optimizer, total_iters, last_epoch=-1):\n","        self.total_iters = total_iters\n","        super().__init__(optimizer, last_epoch)\n","\n","    def get_lr(self):\n","        \"\"\"we will use the first m batches, and set the learning\n","        rate to base_lr * m / total_iters\n","        \"\"\"\n","        return [\n","            base_lr * self.last_epoch / (self.total_iters + 1e-8)\n","            for base_lr in self.base_lrs\n","        ]\n","\n","\n","def most_recent_folder(net_weights, fmt):\n","    \"\"\"\n","    return most recent created folder under net_weights\n","    if no none-empty folder were found, return empty folder\n","    \"\"\"\n","    # get subfolders in net_weights\n","    folders = os.listdir(net_weights)\n","\n","    # filter out empty folders\n","    folders = [f for f in folders if len(os.listdir(os.path.join(net_weights, f)))]\n","    if len(folders) == 0:\n","        return \"\"\n","\n","    # sort folders by folder created time\n","    folders = sorted(folders, key=lambda f: datetime.datetime.strptime(f, fmt))\n","    return folders[-1]\n","\n","\n","def most_recent_weights(weights_folder):\n","    \"\"\"\n","    return most recent created weights file\n","    if folder is empty return empty string\n","    \"\"\"\n","    weight_files = os.listdir(weights_folder)\n","    if len(weights_folder) == 0:\n","        return \"\"\n","\n","    regex_str = r\"([A-Za-z0-9]+)-([0-9]+)-(regular|best)\"\n","\n","    # sort files by epoch\n","    weight_files = sorted(\n","        weight_files, key=lambda w: int(re.search(regex_str, w).groups()[1])\n","    )\n","\n","    return weight_files[-1]\n","\n","\n","def last_epoch(weights_folder):\n","    weight_file = most_recent_weights(weights_folder)\n","    if not weight_file:\n","        raise Exception(\"no recent weights were found\")\n","    resume_epoch = int(weight_file.split(\"-\")[1])\n","\n","    return resume_epoch\n","\n","\n","def best_acc_weights(weights_folder):\n","    \"\"\"\n","    return the best acc .pth file in given folder, if no\n","    best acc weights file were found, return empty string\n","    \"\"\"\n","    files = os.listdir(weights_folder)\n","    if len(files) == 0:\n","        return \"\"\n","\n","    regex_str = r\"([A-Za-z0-9]+)-([0-9]+)-(regular|best)\"\n","    best_files = [w for w in files if re.search(regex_str, w).groups()[2] == \"best\"]\n","    if len(best_files) == 0:\n","        return \"\"\n","\n","    best_files = sorted(\n","        best_files, key=lambda w: int(re.search(regex_str, w).groups()[1])\n","    )\n","    return best_files[-1]"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-06-19T21:53:03.295215Z","iopub.status.busy":"2024-06-19T21:53:03.294909Z","iopub.status.idle":"2024-06-19T21:53:03.308126Z","shell.execute_reply":"2024-06-19T21:53:03.307352Z","shell.execute_reply.started":"2024-06-19T21:53:03.295185Z"},"trusted":true},"outputs":[],"source":["conf = {\n","    \"CHECKPOINT_PATH\": \"checkpoint\",\n","\n","    # Class correspondence as done in https://github.com/vikram2000b/bad-teaching-unlearning\n","    \"class_dict\": {\n","        \"rocket\": 69,\n","        \"vehicle2\": 19,\n","        \"veg\": 4,\n","        \"mushroom\": 51,\n","        \"people\": 14,\n","        \"baby\": 2,\n","        \"electrical_devices\": 5,\n","        \"lamp\": 40,\n","        \"natural_scenes\": 10,\n","        \"sea\": 71,\n","        \"42\": 42,\n","        \"1\": 1,\n","        \"10\": 10,\n","        \"20\": 20,\n","        \"30\": 30,\n","        \"40\": 40,\n","        \"lion\": 43,\n","    },\n","\n","    # Classes from https://github.com/vikram2000b/bad-teaching-unlearning\n","    \"cifar20_classes\": {\"vehicle2\", \"veg\", \"people\", \"electrical_devices\", \"natural_scenes\"},\n","\n","    # Classes from https://github.com/vikram2000b/bad-teaching-unlearning\n","    \"cifar100_classes\": {\"rocket\", \"mushroom\", \"baby\", \"lamp\", \"sea\"},\n","\n","    # total training epochs\n","\n","    # Training parameters for the tasks; milestones are when the learning rate gets lowered\n","\n","    \"Cifar100_EPOCHS\": 200,\n","    \"Cifar100_MILESTONES\": [60, 120, 160],\n","\n","    \"Cifar10_EPOCHS\": 20,\n","    \"Cifar10_MILESTONES\": [8, 12, 16],\n","\n","    \"Cifar20_EPOCHS\": 40,\n","    \"Cifar20_MILESTONES\": [15, 30, 35],\n","\n","    \"Cifar100_EPOCHS\": 200,\n","    \"Cifar100_MILESTONES\":  [60, 120, 160],\n","\n","\n","    \"Cifar10_ViT_EPOCHS\": 8,\n","    \"Cifar10_ViT_MILESTONES\": [7],\n","\n","    \"Cifar20_ViT_EPOCHS\": 9,\n","    \"Cifar20_ViT_MILESTONES\": [8],\n","\n","    \"Cifar100_ViT_EPOCHS\": 8,\n","    \"Cifar100_ViT_MILESTONES\": [7],\n","\n","    # log dir\n","    \"LOG_DIR\": \"runs\",\n","\n","    # save weights file per SAVE_EPOCH epoch\n","    \"SAVE_EPOCH\": 10\n","}\n","\n","DATE_FORMAT = \"%A_%d_%B_%Y_%Hh_%Mm_%Ss\"\n","\n","# time of script run\n","TIME_NOW = datetime.now().strftime(DATE_FORMAT)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-06-19T21:53:03.309979Z","iopub.status.busy":"2024-06-19T21:53:03.309604Z","iopub.status.idle":"2024-06-19T21:53:03.346065Z","shell.execute_reply":"2024-06-19T21:53:03.345261Z","shell.execute_reply.started":"2024-06-19T21:53:03.309948Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","Datasets used for the experiments (CIFAR and HARD)\n","\"\"\"\n","\n","# Improves model performance (https://github.com/weiaicunzai/pytorch-cifar100)\n","CIFAR_MEAN = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343)\n","CIFAR_STD = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)\n","\n","# Cropping etc. to improve performance of the model (details see https://github.com/weiaicunzai/pytorch-cifar100)\n","transform_train_from_scratch = [\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(15),\n","    transforms.ToTensor(),\n","    transforms.Normalize(CIFAR_MEAN, CIFAR_STD),\n","]\n","\n","transform_unlearning = [\n","    # transforms.Resize(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize(CIFAR_MEAN, CIFAR_STD),\n","]\n","\n","transform_test = [\n","    # transforms.Resize(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize(CIFAR_MEAN, CIFAR_STD),\n","]\n","\n","active_transform = None\n","\n","\n","class Cifar100(CIFAR100):\n","    def __init__(self, root, train, unlearning, download, img_size=32):\n","        if train:\n","            if unlearning:\n","                transform = transform_unlearning\n","            else:\n","                transform = transform_train_from_scratch\n","        else:\n","            transform = transform_test\n","        transform = transforms.Compose(transform)\n","        global active_transform\n","        if img_size > 32:\n","            active_transform = transforms.Compose([transforms.Resize((img_size,img_size)), transforms.ConvertImageDtype(torch.float16)])\n","        else:\n","            active_transform = transforms.Resize((img_size,img_size))\n","        self.imgs = {}\n","        super().__init__(root=root, train=train, download=download, transform=transform)\n","\n","    def __getitem__(self, index):\n","        if index in self.imgs:\n","            return self.imgs[index]\n","        x, y = super().__getitem__(index)\n","        self.imgs[index] = x, torch.Tensor([]), y\n","        return self.imgs[index]\n","\n","\n","class Cifar20(CIFAR100):\n","    def __init__(self, root, train, unlearning, download, img_size=32):\n","        if train:\n","            if unlearning:\n","                transform = transform_unlearning\n","            else:\n","                transform = transform_train_from_scratch\n","        else:\n","            transform = transform_test\n","        global active_transform\n","        if img_size > 32:\n","            active_transform = transforms.Compose([transforms.Resize((img_size,img_size)), transforms.ConvertImageDtype(torch.float16)])\n","        else:\n","            active_transform = transforms.Resize((img_size,img_size))\n","        transform = transforms.Compose(transform)\n","\n","        super().__init__(root=root, train=train, download=download, transform=transform)\n","\n","        # This map is for the matching of subclases to the superclasses. E.g., rocket (69) to Vehicle2 (19:)\n","        # Taken from https://github.com/vikram2000b/bad-teaching-unlearning\n","        self.coarse_map = {\n","            0: [4, 30, 55, 72, 95],\n","            1: [1, 32, 67, 73, 91],\n","            2: [54, 62, 70, 82, 92],\n","            3: [9, 10, 16, 28, 61],\n","            4: [0, 51, 53, 57, 83],\n","            5: [22, 39, 40, 86, 87],\n","            6: [5, 20, 25, 84, 94],\n","            7: [6, 7, 14, 18, 24],\n","            8: [3, 42, 43, 88, 97],\n","            9: [12, 17, 37, 68, 76],\n","            10: [23, 33, 49, 60, 71],\n","            11: [15, 19, 21, 31, 38],\n","            12: [34, 63, 64, 66, 75],\n","            13: [26, 45, 77, 79, 99],\n","            14: [2, 11, 35, 46, 98],\n","            15: [27, 29, 44, 78, 93],\n","            16: [36, 50, 65, 74, 80],\n","            17: [47, 52, 56, 59, 96],\n","            18: [8, 13, 48, 58, 90],\n","            19: [41, 69, 81, 85, 89],\n","        }\n","\n","    def __getitem__(self, index):\n","        x, y = super().__getitem__(index)\n","        coarse_y = None\n","        for i in range(20):\n","            for j in self.coarse_map[i]:\n","                if y == j:\n","                    coarse_y = i\n","                    break\n","            if coarse_y != None:\n","                break\n","        if coarse_y == None:\n","            print(y)\n","            assert coarse_y != None\n","        return x, y, coarse_y\n","\n","\n","class Cifar10(CIFAR10):\n","    def __init__(self, root, train, unlearning, download, img_size=32):\n","        if train:\n","            if unlearning:\n","                transform = transform_unlearning\n","            else:\n","                transform = transform_train_from_scratch\n","        else:\n","            transform = transform_test\n","        global active_transform\n","        if img_size > 32:\n","            active_transform = transforms.Compose([transforms.Resize((img_size,img_size)), transforms.ConvertImageDtype(torch.float16)])\n","        else:\n","            active_transform = transforms.Resize((img_size,img_size))\n","        transform = transforms.Compose(transform)\n","        super().__init__(root=root, train=train, download=download, transform=transform)\n","\n","    def __getitem__(self, index):\n","        x, y = super().__getitem__(index)\n","        return x, torch.Tensor([]), y\n","\n","#https://github.com/elnagara/HARD-Arabic-Dataset\n","class HARD():\n","    def __init__(self, root, download, train, unlearning, img_size,file_path):\n","        df = pd.read_csv(file_path)\n","        if(train):\n","            df = df.loc[:40000]\n","        else:\n","            df = df.loc[40000:]\n","        \n","        self.data = df[\"review\"].tolist()\n","        self.targets = df[\"rating\"].tolist()\n","        model_dir = \"/kaggle/input/marbert-hard-data\"\n","        \n","        tokenizer = AutoTokenizer.from_pretrained(model_dir)\n","        \n","        dataset = SingleSentenceClassificationProcessor(mode='classification')\n","        dataset.add_examples(texts_or_text_and_labels=self.data,overwrite_examples = True)\n","        \n","        tokenizer.max_len = 512\n","        self.data = dataset.get_features(tokenizer = tokenizer, max_length =512)\n","        \n","    def __getitem__(self, index):\n","        review = self.data[index]\n","        review_dict = {\"input_ids\":torch.tensor(review.input_ids), \"attention_mask\": torch.tensor(review.attention_mask)}\n","        return (review_dict,self.targets[index])\n","    \n","    def __len__(self):\n","        # Assuming 'data' is a list attribute, return its length\n","        return len(self.data)\n","\n","\n","class UnLearningData(Dataset):\n","    def __init__(self, forget_data, retain_data):\n","        super().__init__()\n","        self.forget_data = forget_data\n","        self.retain_data = retain_data\n","        self.forget_len = len(forget_data)\n","        self.retain_len = len(retain_data)\n","\n","    def __len__(self):\n","        return self.retain_len + self.forget_len\n","\n","    def __getitem__(self, index):\n","        if index < self.forget_len:\n","            x = self.forget_data[index][0]\n","            y = 1\n","            return x, y\n","        else:\n","            x = self.retain_data[index - self.forget_len][0]\n","            y = 0\n","            return x, y"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-06-19T21:53:03.347686Z","iopub.status.busy":"2024-06-19T21:53:03.347419Z","iopub.status.idle":"2024-06-19T21:53:03.374564Z","shell.execute_reply":"2024-06-19T21:53:03.373739Z","shell.execute_reply.started":"2024-06-19T21:53:03.347663Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","From https://github.com/vikram2000b/bad-teaching-unlearning\n","And https://github.com/weiaicunzai/pytorch-cifar100 (better performance) <- Refer to this for comments\n","\"\"\"\n","\n","def ResNet18(num_classes):\n","    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n","\n","\n","class Identity(nn.Module):\n","    def __init__(self):\n","        super(Identity, self).__init__()\n","\n","    def forward(self, x):\n","        return x\n","\n","\n","class Flatten(nn.Module):\n","    def __init__(self):\n","        super(Flatten, self).__init__()\n","\n","    def forward(self, x):\n","        return x.view(x.size(0), -1)\n","\n","\n","class ConvStandard(nn.Conv2d):\n","    def __init__(\n","        self,\n","        in_channels,\n","        out_channels,\n","        kernel_size=3,\n","        stride=1,\n","        padding=None,\n","        output_padding=0,\n","        w_sig=np.sqrt(1.0),\n","    ):\n","        super(ConvStandard, self).__init__(in_channels, out_channels, kernel_size)\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.kernel_size = kernel_size\n","        self.stride = stride\n","        self.padding = padding\n","        self.w_sig = w_sig\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        torch.nn.init.normal_(\n","            self.weight,\n","            mean=0,\n","            std=self.w_sig / (self.in_channels * np.prod(self.kernel_size)),\n","        )\n","        if self.bias is not None:\n","            torch.nn.init.normal_(self.bias, mean=0, std=0)\n","\n","    def forward(self, input):\n","        return F.conv2d(input, self.weight, self.bias, self.stride, self.padding)\n","\n","\n","class Conv(nn.Sequential):\n","    def __init__(\n","        self,\n","        in_channels,\n","        out_channels,\n","        kernel_size=3,\n","        stride=1,\n","        padding=None,\n","        output_padding=0,\n","        activation_fn=nn.ReLU,\n","        batch_norm=True,\n","        transpose=False,\n","    ):\n","        if padding is None:\n","            padding = (kernel_size - 1) // 2\n","        model = []\n","        if not transpose:\n","            #             model += [ConvStandard(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding\n","            #                                 )]\n","            model += [\n","                nn.Conv2d(\n","                    in_channels,\n","                    out_channels,\n","                    kernel_size=kernel_size,\n","                    stride=stride,\n","                    padding=padding,\n","                    bias=not batch_norm,\n","                )\n","            ]\n","        else:\n","            model += [\n","                nn.ConvTranspose2d(\n","                    in_channels,\n","                    out_channels,\n","                    kernel_size,\n","                    stride=stride,\n","                    padding=padding,\n","                    output_padding=output_padding,\n","                    bias=not batch_norm,\n","                )\n","            ]\n","        if batch_norm:\n","            model += [nn.BatchNorm2d(out_channels, affine=True)]\n","        model += [activation_fn()]\n","        super(Conv, self).__init__(*model)\n","\n","\n","class AllCNN(nn.Module):\n","    def __init__(\n","        self,\n","        filters_percentage=1.0,\n","        n_channels=3,\n","        num_classes=10,\n","        dropout=False,\n","        batch_norm=True,\n","    ):\n","        super(AllCNN, self).__init__()\n","        n_filter1 = int(96 * filters_percentage)\n","        n_filter2 = int(192 * filters_percentage)\n","        self.features = nn.Sequential(\n","            Conv(n_channels, n_filter1, kernel_size=3, batch_norm=batch_norm),\n","            Conv(n_filter1, n_filter1, kernel_size=3, batch_norm=batch_norm),\n","            Conv(\n","                n_filter1,\n","                n_filter2,\n","                kernel_size=3,\n","                stride=2,\n","                padding=1,\n","                batch_norm=batch_norm,\n","            ),\n","            nn.Dropout(inplace=True) if dropout else Identity(),\n","            Conv(n_filter2, n_filter2, kernel_size=3, stride=1, batch_norm=batch_norm),\n","            Conv(n_filter2, n_filter2, kernel_size=3, stride=1, batch_norm=batch_norm),\n","            Conv(\n","                n_filter2,\n","                n_filter2,\n","                kernel_size=3,\n","                stride=2,\n","                padding=1,\n","                batch_norm=batch_norm,\n","            ),  # 14\n","            nn.Dropout(inplace=True) if dropout else Identity(),\n","            Conv(n_filter2, n_filter2, kernel_size=3, stride=1, batch_norm=batch_norm),\n","            Conv(n_filter2, n_filter2, kernel_size=1, stride=1, batch_norm=batch_norm),\n","            nn.AvgPool2d(8),\n","            Flatten(),\n","        )\n","        self.classifier = nn.Sequential(\n","            nn.Linear(n_filter2, num_classes),\n","        )\n","\n","    def forward(self, x):\n","        features = self.features(x)\n","        output = self.classifier(features)\n","        return output\n","\n","\n","class ViT(nn.Module):\n","    def __init__(self, num_classes=20, **kwargs):\n","        super(ViT, self).__init__()\n","        self.base = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n","        self.final = nn.Linear(self.base.config.hidden_size, num_classes)\n","        self.num_classes = num_classes\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, pixel_values):\n","        outputs = self.base(pixel_values=pixel_values)\n","        logits = self.final(outputs.last_hidden_state[:, 0])\n","\n","        return logits\n","    \n","class Marbert(nn.Module):\n","    def __init__(self, num_classes=5, model_dir=\"\", **kwargs):\n","        super(Marbert, self).__init__()\n","       \n","        config = AutoConfig.from_pretrained(model_dir, num_labels=num_classes)\n","        \n","        self.base = AutoModelForSequenceClassification.from_pretrained(model_dir, config = config)\n","        self.num_classes = num_classes\n","        self.config = config\n","    \n","    def forward(self, input_ids, attention_mask):\n","        # Forward pass computation\n","        outputs = self.base(input_ids=input_ids, attention_mask=attention_mask)\n","        return outputs.logits\n","    \n","\n","    \n","    def __call__(self, reviews):\n","        return super(Marbert, self).__call__(reviews['input_ids'].to(\"cuda\"), reviews['attention_mask'].to(\"cuda\")).to(\"cuda\")"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-06-19T21:53:03.376487Z","iopub.status.busy":"2024-06-19T21:53:03.375675Z","iopub.status.idle":"2024-06-19T21:53:03.389288Z","shell.execute_reply":"2024-06-19T21:53:03.388406Z","shell.execute_reply.started":"2024-06-19T21:53:03.376456Z"},"trusted":true},"outputs":[],"source":["# Original code from https://github.com/weiaicunzai/pytorch-cifar100 <- refer to this repo for comments\n","\n","\n","def train(epochs):\n","    start = time.time()\n","    net.train()\n","    for batch_index, (images, _, labels) in enumerate(trainloader):\n","        if args[\"gpu\"]:\n","            labels = labels.cuda()\n","            images = images.cuda()\n","\n","        optimizer.zero_grad()\n","        outputs = net(images)\n","        loss = loss_function(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # print('Training Epoch: {epoch} [{trained_samples}/{total_samples}]\\tLoss: {:0.4f}\\tLR: {:0.6f}'.format(\n","        #     loss.item(),\n","        #     optimizer.param_groups[0]['lr'],\n","        #     epoch=epoch,\n","        #     trained_samples=batch_index * args.b + len(images),\n","        #     total_samples=len(trainloader.dataset)\n","        # ))\n","\n","        if epoch <= args[\"warm\"]:\n","            warmup_scheduler.step()\n","\n","    finish = time.time()\n","\n","    print(\"epoch {} training time consumed: {:.2f}s\".format(epoch, finish - start))\n","\n","\n","@torch.no_grad()\n","def eval_training(epoch=0, tb=True):\n","    start = time.time()\n","    net.eval()\n","\n","    test_loss = 0.0  # cost function error\n","    correct = 0.0\n","\n","    for images, _, labels in testloader:\n","        if args[\"gpu\"]:\n","            images = images.cuda()\n","            labels = labels.cuda()\n","\n","        outputs = net(images)\n","        loss = loss_function(outputs, labels)\n","\n","        test_loss += loss.item()\n","        _, preds = outputs.max(1)\n","        correct += preds.eq(labels).sum()\n","\n","    finish = time.time()\n","    if args[\"gpu\"]:\n","        print(\"GPU INFO.....\")\n","        print(torch.cuda.memory_summary(), end=\"\")\n","    print(\"Evaluating Network.....\")\n","    print(\n","        \"Test set: Epoch: {}, Average loss: {:.4f}, Accuracy: {:.4f}, Time consumed:{:.2f}s\".format(\n","            epoch,\n","            test_loss / len(testloader.dataset),\n","            correct.float() / len(testloader.dataset),\n","            finish - start,\n","        )\n","    )\n","    print()\n","\n","    return correct.float() / len(testloader.dataset)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-06-19T21:53:03.390637Z","iopub.status.busy":"2024-06-19T21:53:03.390323Z","iopub.status.idle":"2024-06-19T21:53:03.401856Z","shell.execute_reply":"2024-06-19T21:53:03.401111Z","shell.execute_reply.started":"2024-06-19T21:53:03.390607Z"},"trusted":true},"outputs":[],"source":["datasets_dict = {\n","    \"Cifar10\": Cifar10,\n","    \"Cifar20\": Cifar20,\n","    \"Cifar100\": Cifar100,\n","    \"HARD\": HARD\n","}"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-06-19T21:53:03.452355Z","iopub.status.busy":"2024-06-19T21:53:03.452101Z","iopub.status.idle":"2024-06-19T21:53:03.476426Z","shell.execute_reply":"2024-06-19T21:53:03.475588Z","shell.execute_reply.started":"2024-06-19T21:53:03.452333Z"},"trusted":true},"outputs":[],"source":["# From https://github.com/vikram2000b/bad-teaching-unlearning\n","\n","def accuracy(outputs, labels):\n","    _, preds = torch.max(outputs, dim=1)\n","    return torch.tensor(torch.sum(preds == labels).item() / len(preds)) * 100\n","\n","\n","def training_step(model, batch, device, nlp=False):\n","    if not nlp:\n","        images, labels, clabels = batch\n","        new_images = []\n","        for image in images:\n","            new_images.append(active_transform(image))\n","        new_images = torch.stack(new_images)\n","        images, clabels = new_images.to(device), clabels.to(device)\n","        out = model(images)  # Generate predictions\n","        loss = F.cross_entropy(out, clabels)  # Calculate loss\n","        return loss\n","    else:\n","        review, rating = batch\n","        rating = rating.to(\"cuda\")\n","        out = model(review)  # Generate predictions\n","        loss = F.cross_entropy(out, rating)  # Calculate loss\n","        return loss\n","\n","\n","def validation_step(model, batch, device, nlp=False):\n","    if not nlp:\n","        images, labels, clabels = batch\n","        new_images = []\n","        for image in images:\n","            new_images.append(active_transform(image))\n","        new_images = torch.stack(new_images)\n","        images, clabels = new_images.to(device), clabels.to(device)\n","        out = model(images)  # Generate predictions\n","        loss = F.cross_entropy(out, clabels)  # Calculate loss\n","        acc = accuracy(out, clabels)  # Calculate accuracy\n","        return {\"Loss\": loss.detach(), \"Acc\": acc}\n","    else:\n","        review, rating = batch\n","        rating = rating.to(\"cuda\")\n","        out = model(review)  # Generate predictions\n","        loss = F.cross_entropy(out, rating)  # Calculate loss\n","        acc = accuracy(out, rating)  # Calculate accuracy\n","        return {\"Loss\": loss.detach(), \"Acc\": acc}\n","\n","\n","def validation_epoch_end(model, outputs):\n","    batch_losses = [x[\"Loss\"] for x in outputs]\n","    epoch_loss = torch.stack(batch_losses).mean()  # Combine losses\n","    batch_accs = [x[\"Acc\"] for x in outputs]\n","    epoch_acc = torch.stack(batch_accs).mean()  # Combine accuracies\n","    return {\"Loss\": epoch_loss.item(), \"Acc\": epoch_acc.item()}\n","\n","\n","def epoch_end(model, epoch, result):\n","    print(\n","        \"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n","            epoch,\n","            result[\"lrs\"][-1],\n","            result[\"train_loss\"],\n","            result[\"Loss\"],\n","            result[\"Acc\"],\n","        )\n","    )\n","\n","\n","@torch.no_grad()\n","def evaluate(model, val_loader, device, nlp=False):\n","    model.eval()\n","    outputs = [validation_step(model, batch, device, nlp) for batch in val_loader]\n","    return validation_epoch_end(model, outputs)\n","\n","\n","def get_lr(optimizer):\n","    for param_group in optimizer.param_groups:\n","        return param_group[\"lr\"]\n","\n","\n","def fit_one_cycle(\n","    epochs, model, train_loader, val_loader, device, lr=0.01, milestones=None\n","):\n","    torch.cuda.empty_cache()\n","    history = []\n","\n","    optimizer = torch.optim.SGD(model.parameters(), lr, momentum=0.9, weight_decay=5e-4)\n","    if milestones:\n","        train_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n","            optimizer, milestones=milestones, gamma=0.2\n","        )  # learning rate decay\n","        warmup_scheduler = WarmUpLR(optimizer, len(train_loader))\n","\n","    for epoch in range(epochs):\n","        if epoch > 1 and milestones:\n","            train_scheduler.step(epoch)\n","\n","        model.train()\n","        train_losses = []\n","        lrs = []\n","        for batch in train_loader:\n","            loss = training_step(model, batch, device)\n","            train_losses.append(loss)\n","            loss.backward()\n","\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","            lrs.append(get_lr(optimizer))\n","\n","            if epoch <= 1 and milestones:\n","                warmup_scheduler.step()\n","\n","        # Validation phase\n","        result = evaluate(model, val_loader, device)\n","        result[\"train_loss\"] = torch.stack(train_losses).mean().item()\n","        result[\"lrs\"] = lrs\n","        epoch_end(model, epoch, result)\n","        history.append(result)\n","    return history\n","\n","def fit_one_unlearning_cycle(epochs, model, train_loader, val_loader, lr, device, nlp=False):\n","    history = []\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        train_losses = []\n","        lrs = []\n","        for batch in train_loader:\n","            loss = training_step(model, batch, device, nlp)\n","            loss.backward()\n","            train_losses.append(loss.detach().cpu())\n","\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","            lrs.append(get_lr(optimizer))\n","\n","        result = evaluate(model, val_loader, device, nlp)\n","        result[\"train_loss\"] = torch.stack(train_losses).mean()\n","        result[\"lrs\"] = lrs\n","        epoch_end(model, epoch, result)\n","        history.append(result)\n","    return history"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-06-19T21:53:03.478001Z","iopub.status.busy":"2024-06-19T21:53:03.477619Z","iopub.status.idle":"2024-06-19T21:53:03.502071Z","shell.execute_reply":"2024-06-19T21:53:03.501156Z","shell.execute_reply.started":"2024-06-19T21:53:03.477971Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","From https://github.com/vikram2000b/bad-teaching-unlearning / https://arxiv.org/abs/2205.08096\n","\"\"\"\n","\n","def JSDiv(p, q):\n","    m = (p + q) / 2\n","    return 0.5 * F.kl_div(torch.log(p), m) + 0.5 * F.kl_div(torch.log(q), m)\n","\n","\n","# ZRF/UnLearningScore https://arxiv.org/abs/2205.08096\n","def UnLearningScore(tmodel, gold_model, forget_dl, batch_size, device, nlp=False):\n","    model_preds = []\n","    gold_model_preds = []\n","    with torch.no_grad():\n","        if not nlp:\n","            for batch in forget_dl:\n","                x, y, cy = batch\n","                new_x = []\n","                for i in x:\n","                    new_x.append(active_transform(i))\n","                new_x = torch.stack(new_x)\n","                x = new_x.to(device)\n","                model_output = tmodel(x)\n","                gold_model_output = gold_model(x)\n","                model_preds.append(F.softmax(model_output, dim=1).detach().cpu())\n","                gold_model_preds.append(F.softmax(gold_model_output, dim=1).detach().cpu())\n","        else:\n","            for batch in forget_dl:\n","                x, y = batch\n","                model_output = tmodel(x)\n","                gold_model_output = gold_model(x)\n","                model_preds.append(F.softmax(model_output, dim=1).detach().cpu())\n","                gold_model_preds.append(F.softmax(gold_model_output, dim=1).detach().cpu())\n","\n","    model_preds = torch.cat(model_preds, axis=0)\n","    gold_model_preds = torch.cat(gold_model_preds, axis=0)\n","    return 1 - JSDiv(model_preds, gold_model_preds)\n","\n","\n","def entropy(p, dim=-1, keepdim=False):\n","    return -torch.where(p > 0, p * p.log(), p.new([0.0])).sum(dim=dim, keepdim=keepdim)\n","\n","\n","def collect_prob(data_loader, model, nlp=False):\n","    if not nlp:\n","        data_loader = torch.utils.data.DataLoader(\n","            data_loader.dataset, batch_size=1, shuffle=False\n","        )\n","        prob = []\n","        with torch.no_grad():\n","            for batch in data_loader:\n","                data, _, target = batch\n","                new_data = []\n","                for i in data:\n","                    new_data.append(active_transform(i))\n","                new_data = torch.stack(new_data)\n","                data = new_data.to(next(model.parameters()).device)\n","                output = model(data)\n","                prob.append(F.softmax(output, dim=-1).data)\n","        return torch.cat(prob)\n","    else:\n","        data_loader = torch.utils.data.DataLoader(\n","            data_loader.dataset, batch_size=1, shuffle=False\n","        )\n","        prob = []\n","        with torch.no_grad():\n","            for batch in data_loader:\n","                data,target = batch\n","                output = model(data)\n","                prob.append(F.softmax(output, dim=-1).data)\n","        return torch.cat(prob)\n","\n","\n","# https://arxiv.org/abs/2205.08096\n","def get_membership_attack_data(retain_loader, forget_loader, test_loader, model, nlp=False):\n","    retain_prob = collect_prob(retain_loader, model, nlp)\n","    forget_prob = collect_prob(forget_loader, model, nlp)\n","    test_prob = collect_prob(test_loader, model, nlp)\n","\n","    X_r = (\n","        torch.cat([entropy(retain_prob), entropy(test_prob)])\n","        .cpu()\n","        .numpy()\n","        .reshape(-1, 1)\n","    )\n","    Y_r = np.concatenate([np.ones(len(retain_prob)), np.zeros(len(test_prob))])\n","\n","    X_f = entropy(forget_prob).cpu().numpy().reshape(-1, 1)\n","    Y_f = np.concatenate([np.ones(len(forget_prob))])\n","    return X_f, Y_f, X_r, Y_r\n","\n","\n","# https://arxiv.org/abs/2205.08096\n","def get_membership_attack_prob(retain_loader, forget_loader, test_loader, model, nlp=False):\n","    X_f, Y_f, X_r, Y_r = get_membership_attack_data(\n","        retain_loader, forget_loader, test_loader, model, nlp\n","    )\n","    # clf = SVC(C=3,gamma='auto',kernel='rbf')\n","    clf = LogisticRegression(\n","        class_weight=\"balanced\", solver=\"lbfgs\", multi_class=\"multinomial\"\n","    )\n","    clf.fit(X_r, Y_r)\n","    results = clf.predict(X_f)\n","    return results.mean()\n","\n","\n","@torch.no_grad()\n","def actv_dist(model1, model2, dataloader, device=\"cuda\"):\n","    sftmx = nn.Softmax(dim=1)\n","    distances = []\n","    for batch in dataloader:\n","        x, _, _ = batch\n","        x = x.to(device)\n","        model1_out = model1(x)\n","        model2_out = model2(x)\n","        diff = torch.sqrt(\n","            torch.sum(\n","                torch.square(\n","                    F.softmax(model1_out, dim=1) - F.softmax(model2_out, dim=1)\n","                ),\n","                axis=1,\n","            )\n","        )\n","        diff = diff.detach().cpu()\n","        distances.append(diff)\n","    distances = torch.cat(distances, axis=0)\n","    return distances.mean()"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-06-19T21:53:03.503342Z","iopub.status.busy":"2024-06-19T21:53:03.503092Z","iopub.status.idle":"2024-06-19T21:53:03.555444Z","shell.execute_reply":"2024-06-19T21:53:03.554533Z","shell.execute_reply.started":"2024-06-19T21:53:03.503320Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","This file is used for the Selective Synaptic Dampening method\n","https://github.com/if-loops/selective-synaptic-dampening\n","\"\"\"\n","\n","###############################################\n","# Clean implementation\n","###############################################\n","\n","\n","class ParameterPerturber:\n","    def __init__(\n","        self,\n","        model,\n","        opt,\n","        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n","        parameters=None,\n","    ):\n","        self.model = model\n","        self.opt = opt\n","        self.device = device\n","        self.alpha = None\n","        self.xmin = None\n","\n","        print(parameters)\n","        self.lower_bound = parameters[\"lower_bound\"]\n","        self.exponent = parameters[\"exponent\"]\n","        self.magnitude_diff = parameters[\"magnitude_diff\"]  # unused\n","        self.min_layer = parameters[\"min_layer\"]\n","        self.max_layer = parameters[\"max_layer\"]\n","        self.forget_threshold = parameters[\"forget_threshold\"]\n","        self.dampening_constant = parameters[\"dampening_constant\"]\n","        self.selection_weighting = parameters[\"selection_weighting\"]\n","\n","    def get_layer_num(self, layer_name: str) -> int:\n","        layer_id = layer_name.split(\".\")[1]\n","        if layer_id.isnumeric():\n","            return int(layer_id)\n","        else:\n","            return -1\n","\n","    def zerolike_params_dict(self, model: torch.nn) -> Dict[str, torch.Tensor]:\n","        \"\"\"\n","        Taken from: Avalanche: an End-to-End Library for Continual Learning - https://github.com/ContinualAI/avalanche\n","        Returns a dict like named_parameters(), with zeroed-out parameter valuse\n","        Parameters:\n","        model (torch.nn): model to get param dict from\n","        Returns:\n","        dict(str,torch.Tensor): dict of zero-like params\n","        \"\"\"\n","        return dict(\n","            [\n","                (k, torch.zeros_like(p, device=p.device))\n","                for k, p in model.named_parameters()\n","            ]\n","        )\n","\n","    def fulllike_params_dict(\n","        self, model: torch.nn, fill_value, as_tensor: bool = False\n","    ) -> Dict[str, torch.Tensor]:\n","        \"\"\"\n","        Returns a dict like named_parameters(), with parameter values replaced with fill_value\n","\n","        Parameters:\n","        model (torch.nn): model to get param dict from\n","        fill_value: value to fill dict with\n","        Returns:\n","        dict(str,torch.Tensor): dict of named_parameters() with filled in values\n","        \"\"\"\n","\n","        def full_like_tensor(fillval, shape: list) -> list:\n","            \"\"\"\n","            recursively builds nd list of shape shape, filled with fillval\n","            Parameters:\n","            fillval: value to fill matrix with\n","            shape: shape of target tensor\n","            Returns:\n","            list of shape shape, filled with fillval at each index\n","            \"\"\"\n","            if len(shape) > 1:\n","                fillval = full_like_tensor(fillval, shape[1:])\n","            tmp = [fillval for _ in range(shape[0])]\n","            return tmp\n","\n","        dictionary = {}\n","\n","        for n, p in model.named_parameters():\n","            _p = (\n","                torch.tensor(full_like_tensor(fill_value, p.shape), device=self.device)\n","                if as_tensor\n","                else full_like_tensor(fill_value, p.shape)\n","            )\n","            dictionary[n] = _p\n","        return dictionary\n","\n","    def subsample_dataset(self, dataset: dataset, sample_perc: float) -> Subset:\n","        \"\"\"\n","        Take a subset of the dataset\n","\n","        Parameters:\n","        dataset (dataset): dataset to be subsampled\n","        sample_perc (float): percentage of dataset to sample. range(0,1)\n","        Returns:\n","        Subset (float): requested subset of the dataset\n","        \"\"\"\n","        sample_idxs = np.arange(0, len(dataset), step=int((1 / sample_perc)))\n","        return Subset(dataset, sample_idxs)\n","\n","    def split_dataset_by_class(self, dataset: dataset) -> List[Subset]:\n","        \"\"\"\n","        Split dataset into list of subsets\n","            each idx corresponds to samples from that class\n","\n","        Parameters:\n","        dataset (dataset): dataset to be split\n","        Returns:\n","        subsets (List[Subset]): list of subsets of the dataset,\n","            each containing only the samples belonging to that class\n","        \"\"\"\n","        n_classes = len(set([target for _, target in dataset]))\n","        subset_idxs = [[] for _ in range(n_classes)]\n","        for idx, (x, y) in enumerate(dataset):\n","            subset_idxs[y].append(idx)\n","\n","        return [Subset(dataset, subset_idxs[idx]) for idx in range(n_classes)]\n","\n","    def calc_importance(self, dataloader: DataLoader, nlp=False) -> Dict[str, torch.Tensor]:\n","        \"\"\"\n","        Adapated from: Avalanche: an End-to-End Library for Continual Learning - https://github.com/ContinualAI/avalanche\n","        Calculate per-parameter, importance\n","            returns a dictionary [param_name: list(importance per parameter)]\n","        Parameters:\n","        DataLoader (DataLoader): DataLoader to be iterated over\n","        Returns:\n","        importances (dict(str, torch.Tensor([]))): named_parameters-like dictionary containing list of importances for each parameter\n","        \"\"\"\n","        criterion = nn.CrossEntropyLoss()\n","        importances = self.zerolike_params_dict(self.model)\n","        if not nlp:\n","            for batch in dataloader:\n","                x, _, y = batch\n","                new_x = []\n","                for image in x:\n","                    new_x.append(active_transform(image))\n","                new_x = torch.stack(new_x)\n","                x, y = new_x.to(self.device), y.to(self.device)\n","                self.opt.zero_grad()\n","                out = self.model(x)\n","                loss = criterion(out, y)\n","                loss.backward()\n","\n","                for (k1, p), (k2, imp) in zip(\n","                    self.model.named_parameters(), importances.items()\n","                ):\n","                    if p.grad is not None:\n","                        imp.data += p.grad.data.clone().pow(2)\n","        else:\n","            for batch in dataloader:\n","                x, y = batch\n","                y = y.to(self.device)\n","                self.opt.zero_grad()\n","                out = self.model(x)\n","                loss = criterion(out, y)\n","                loss.backward()\n","\n","                for (k1, p), (k2, imp) in zip(\n","                    self.model.named_parameters(), importances.items()\n","                ):\n","                    if p.grad is not None:\n","                        imp.data += p.grad.data.clone().pow(2)\n","\n","        # average over mini batch length\n","        for _, imp in importances.items():\n","            imp.data /= float(len(dataloader))\n","        return importances\n","\n","    def modify_weight(\n","        self,\n","        original_importance: List[Dict[str, torch.Tensor]],\n","        forget_importance: List[Dict[str, torch.Tensor]],\n","    ) -> None:\n","        \"\"\"\n","        Perturb weights based on the SSD equations given in the paper\n","        Parameters:\n","        original_importance (List[Dict[str, torch.Tensor]]): list of importances for original dataset\n","        forget_importance (List[Dict[str, torch.Tensor]]): list of importances for forget sample\n","        threshold (float): value to multiply original imp by to determine memorization.\n","\n","        Returns:\n","        None\n","\n","        \"\"\"\n","\n","        with torch.no_grad():\n","            for (n, p), (oimp_n, oimp), (fimp_n, fimp) in zip(\n","                self.model.named_parameters(),\n","                original_importance.items(),\n","                forget_importance.items(),\n","            ):\n","                # Synapse Selection with parameter alpha\n","                oimp_norm = oimp.mul(self.selection_weighting)\n","                locations = torch.where(fimp > oimp_norm)\n","\n","                # Synapse Dampening with parameter lambda\n","                weight = ((oimp.mul(self.dampening_constant)).div(fimp)).pow(\n","                    self.exponent\n","                )\n","                update = weight[locations]\n","                # Bound by 1 to prevent parameter values to increase.\n","                min_locs = torch.where(update > self.lower_bound)\n","                update[min_locs] = self.lower_bound\n","                p[locations] = p[locations].mul(update)\n","\n","\n","###############################################"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-06-19T21:53:03.557054Z","iopub.status.busy":"2024-06-19T21:53:03.556752Z","iopub.status.idle":"2024-06-19T21:53:03.592116Z","shell.execute_reply":"2024-06-19T21:53:03.591358Z","shell.execute_reply.started":"2024-06-19T21:53:03.557031Z"},"trusted":true},"outputs":[],"source":["def get_metric_scores(\n","    model,\n","    unlearning_teacher,\n","    retain_train_dl,\n","    retain_valid_dl,\n","    forget_train_dl,\n","    forget_valid_dl,\n","    valid_dl,\n","    device\n","):\n","    loss_acc_dict = evaluate(model, valid_dl, device)\n","    retain_acc_dict = evaluate(model, retain_valid_dl, device)\n","    zrf = UnLearningScore(model, unlearning_teacher, forget_valid_dl, 128, device)\n","    d_f = evaluate(model, forget_valid_dl, device)\n","    mia = get_membership_attack_prob(retain_train_dl, forget_train_dl, valid_dl, model)\n","\n","    return (loss_acc_dict[\"Acc\"], retain_acc_dict[\"Acc\"], zrf, mia, d_f[\"Acc\"])\n","\n","\n","def baseline(\n","    model,\n","    unlearning_teacher,\n","    retain_train_dl,\n","    retain_valid_dl,\n","    forget_train_dl,\n","    forget_valid_dl,\n","    valid_dl,\n","    device,\n","    **kwargs,\n","):\n","    return get_metric_scores(\n","        model,\n","        unlearning_teacher,\n","        retain_train_dl,\n","        retain_valid_dl,\n","        forget_train_dl,\n","        forget_valid_dl,\n","        valid_dl,\n","        device,\n","    )\n","\n","\n","def retrain(\n","    model,\n","    unlearning_teacher,\n","    retain_train_dl,\n","    retain_valid_dl,\n","    forget_train_dl,\n","    forget_valid_dl,\n","    valid_dl,\n","    dataset_name,\n","    model_name,\n","    device,\n","    **kwargs,\n","):\n","    for layer in model.children():\n","        if hasattr(layer, \"reset_parameters\"):\n","            layer.reset_parameters()\n","    if model_name == \"ViT\":\n","        epochs = conf[f\"{dataset_name}_{model_name}_EPOCHS\"]\n","        milestones = conf[f\"{dataset_name}_{model_name}_MILESTONES\"]\n","    else:\n","        epochs = conf[f\"{dataset_name}_EPOCHS\"]\n","        milestones = conf[f\"{dataset_name}_MILESTONES\"]\n","    if \"epochs\" in kwargs:\n","        epochs = kwargs[\"epochs\"]\n","    _ = fit_one_cycle(\n","        epochs,\n","        model,\n","        retain_train_dl,\n","        retain_valid_dl,\n","        lr=kwargs[\"lr\"],\n","        milestones=milestones,\n","        device=device,\n","    )\n","\n","    return get_metric_scores(\n","        model,\n","        unlearning_teacher,\n","        retain_train_dl,\n","        retain_valid_dl,\n","        forget_train_dl,\n","        forget_valid_dl,\n","        valid_dl,\n","        device,\n","    )\n","\n","def amnesiac(\n","    model,\n","    unlearning_teacher,\n","    retain_train_dl,\n","    retain_valid_dl,\n","    forget_train_dl,\n","    forget_valid_dl,\n","    valid_dl,\n","    num_classes,\n","    device,\n","    **kwargs,\n","):\n","    unlearninglabels = list(range(num_classes))\n","    unlearning_trainset = []\n","\n","    for x, _, clabel in forget_train_dl.dataset:\n","        rnd = random.choice(unlearninglabels)\n","        while rnd == clabel:\n","            rnd = random.choice(unlearninglabels)\n","        unlearning_trainset.append((x, _, rnd))\n","\n","    for x, _, y in retain_train_dl.dataset:\n","        unlearning_trainset.append((x, _, y))\n","\n","    unlearning_train_set_dl = DataLoader(\n","        unlearning_trainset, kwargs[\"batch_size\"], pin_memory=True, shuffle=True\n","    )\n","\n","    _ = fit_one_unlearning_cycle(\n","        kwargs[\"epochs\"], model, unlearning_train_set_dl, retain_valid_dl, device=device, lr=kwargs[\"lr\"]\n","    )\n","    return get_metric_scores(\n","        model,\n","        unlearning_teacher,\n","        retain_train_dl,\n","        retain_valid_dl,\n","        forget_train_dl,\n","        forget_valid_dl,\n","        valid_dl,\n","        device,\n","    )\n","\n","\n","def ssd_tuning(\n","    model,\n","    unlearning_teacher,\n","    retain_train_dl,\n","    retain_valid_dl,\n","    forget_train_dl,\n","    forget_valid_dl,\n","    valid_dl,\n","    dampening_constant,\n","    selection_weighting,\n","    full_train_dl,\n","    device,\n","    **kwargs,\n","):\n","    parameters = {\n","        \"lower_bound\": 1,\n","        \"exponent\": 1,\n","        \"magnitude_diff\": None,\n","        \"min_layer\": -1,\n","        \"max_layer\": -1,\n","        \"forget_threshold\": 1,\n","        \"dampening_constant\": dampening_constant,\n","        \"selection_weighting\": selection_weighting,\n","    }\n","\n","    # load the trained model\n","    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n","\n","    ssd = ParameterPerturber(model, optimizer, device, parameters)\n","    model = model.eval()\n","\n","    sample_importances = ssd.calc_importance(forget_train_dl)\n","\n","    original_importances = ssd.calc_importance(full_train_dl)\n","    ssd.modify_weight(original_importances, sample_importances)\n","    return get_metric_scores(\n","        model,\n","        unlearning_teacher,\n","        retain_train_dl,\n","        retain_valid_dl,\n","        forget_train_dl,\n","        forget_valid_dl,\n","        valid_dl,\n","        device,\n","    )"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-06-19T21:53:03.593387Z","iopub.status.busy":"2024-06-19T21:53:03.593122Z","iopub.status.idle":"2024-06-19T21:53:03.609598Z","shell.execute_reply":"2024-06-19T21:53:03.608801Z","shell.execute_reply.started":"2024-06-19T21:53:03.593364Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'dataset = \"Cifar10\"\\nn_classes = 10\\nweight_path = \"/kaggle/input/vit-cifar10/ViT-Cifar10-6-best.pth\"\\n\\nargs = {\\n        \"net\": ViT,\\n        \"model_name\": \"ViT\",\\n        \"weight_path\": weight_path,\\n        # choices=[\"Cifar10\", \"Cifar20\", \"Cifar100\", \"PinsFaceRecognition\"]\\n        \"dataset\": dataset,\\n        \"classes\": n_classes,\\n        \"gpu\": True,\\n        \"b\": 64,\\n        \"warm\": 1,\\n        \"lr\": 0.0002,\\n        # choices=[\"baseline\",\"retrain\",\"finetune\",\"blindspot\",\"amnesiac\",\"FisherForgetting\",\\n        #    \"ssd_tuning\",]\\n        \"method\": retrain,\\n        \"epochs\": 1,\\n        \"seed\": 0\\n}\\n\\n# Set seeds\\ntorch.manual_seed(args[\"seed\"])\\nnp.random.seed(args[\"seed\"])\\nrandom.seed(args[\"seed\"])\\n\\n\\nbatch_size = args[\"b\"]\\n\\n# get network\\nnet = args[\"net\"](num_classes=args[\"classes\"])\\nnet.load_state_dict(torch.load(args[\"weight_path\"]))\\n\\nunlearning_teacher = args[\"net\"](num_classes=args[\"classes\"])\\n\\nif args[\"gpu\"]:\\n    net = net.cuda()\\n    unlearning_teacher = unlearning_teacher.cuda()\\n\\n\\nroot = \"105_classes_pins_dataset\" if args[\"dataset\"] == \"PinsFaceRecognition\" else \"./data\"\\n\\n\\nimg_size = 224 if args[\"net\"] == ViT else 32\\ntrainset = datasets_dict[args[\"dataset\"]](\\n    root=root, download=True, train=True, unlearning=True, img_size=img_size\\n)\\nvalidset = datasets_dict[args[\"dataset\"]](\\n    root=root, download=True, train=False, unlearning=True, img_size=img_size\\n)\\n\\ntrainloader = DataLoader(trainset, num_workers=4, batch_size=args[\"b\"], shuffle=True)\\nvalidloader = DataLoader(validset, num_workers=4, batch_size=args[\"b\"], shuffle=False)\\n\\n# for loop runs out of memory here :^(\\n#for forget_perc in [0.05, 0.15, 0.25, 0.35, 0.45]:\\nfor forget_perc in [0.45]:\\n    forget_train, retain_train = torch.utils.data.random_split(\\n        trainset, [forget_perc, 1 - forget_perc]\\n    )\\n    forget_train_dl = DataLoader(list(forget_train), batch_size=args[\"b\"])\\n    retain_train_dl = DataLoader(list(retain_train), batch_size=args[\"b\"], shuffle=True)\\n    forget_valid_dl = forget_train_dl\\n    retain_valid_dl = validloader\\n\\n    model_size_scaler = 1\\n    \\n    # Change alpha here as described in the paper\\n    if args[\"net\"] == ViT:\\n        model_size_scaler = 2.5\\n    else:\\n        model_size_scaler = 1\\n\\n\\n    full_train_dl = DataLoader(\\n        ConcatDataset((retain_train_dl.dataset, forget_train_dl.dataset)),\\n        batch_size=batch_size,\\n    )\\n\\n    kwargs = {\\n        \"model\": net,\\n        \"unlearning_teacher\": unlearning_teacher,\\n        \"retain_train_dl\": retain_train_dl,\\n        \"retain_valid_dl\": retain_valid_dl,\\n        \"forget_train_dl\": forget_train_dl,\\n        \"forget_valid_dl\": forget_valid_dl,\\n        \"full_train_dl\": full_train_dl,\\n        \"valid_dl\": validloader,\\n        \"lr\": args[\"lr\"],\\n        \"dampening_constant\": 1,\\n        \"selection_weighting\": 10 * model_size_scaler,\\n        \"num_classes\": args[\"classes\"],\\n        \"dataset_name\": args[\"dataset\"],\\n        \"device\": \"cuda\" if args[\"gpu\"] else \"cpu\",\\n        \"model_name\": args[\"model_name\"],\\n    }\\n\\n    start = time.time()\\n\\n    testacc, retainacc, zrf, mia, d_f = args[\"method\"](\\n        **kwargs\\n    )\\n    end = time.time()\\n    time_elapsed = end - start\\n\\n    res_dict = {\\n            \"TestAcc\": testacc,\\n            \"RetainTestAcc\": retainacc,\\n            \"ZRF\": zrf,\\n            \"MIA\": mia,\\n            \"Df\": d_f,\\n            \"model_scaler\": model_size_scaler,\\n            \"MethodTime\": time_elapsed,  # do not forget to deduct baseline time from it to remove results calc (acc, MIA, ...)\\n        }\\n\\n    for k,v in res_dict.items():\\n        print(k + \": \" + str(v))\\n'"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["# This cell initiates the random forgetting process for the provided arguments.\n","# Setting the arguments to be used below changes the output correspondingly.\n","\n","args = {\n","        # The name of the network to be used without quotations.\n","        # choices = [ResNet18, ViT]\n","        \"net\": ViT,\n","        # The name of the network to be used with quotations.\n","        # choices = [\"ResNet18\", \"ViT\"]\n","        \"model_name\": \"ViT\",\n","        # Path to the saved model weights file to load model weights from.\n","        \"weight_path\": \"/kaggle/input/vit-cifar10/ViT-Cifar10-6-best.pth\",\n","        # Dataset name to use for the random forgetting experiment with quotations.\n","        # choices = [\"Cifar10\"]\n","        \"dataset\": \"Cifar10\",\n","        # Number of classes predicted by the base model\n","        \"classes\": 10,\n","        # Percentage of the dataset to attempt to forget.\n","        # 0.00 <= forget_perc < 1.00 \n","        \"forget_perc\": 0.05,\n","        # Whether to use GPU acceleration.\n","        \"gpu\": True,\n","        # Batch size to use.\n","        \"b\": 64,\n","        # Warming parameter for optimizer scheduling.\n","        \"warm\": 1,\n","        # Learning rate for retrain-based methods.\n","        \"lr\": 0.0002,\n","        # Alpha hyperparameter value to use for Selective Synaptive Dampening.\n","        # Does not affect other methods.\n","        \"alpha\": 25,\n","        # Gamma hyperparameter value to use for Selective Synaptive Dampening.\n","        # Does not affect other methods.\n","        \"gamma\": 1.0,\n","        # Name of method to apply for unlearning without quotations.\n","        # choices=[baseline, retrain, amnesiac, ssd_tuning]\n","        \"method\": ssd_tuning,\n","        # Number of epochs for retrain-based methods.\n","        \"epochs\": 1,\n","        # Random seed number to use.\n","        \"seed\": 0\n","}\n","\n","# Set seeds\n","torch.manual_seed(args[\"seed\"])\n","np.random.seed(args[\"seed\"])\n","random.seed(args[\"seed\"])\n","\n","\n","batch_size = args[\"b\"]\n","forget_perc = args[\"forget_perc\"]\n","\n","# get network\n","net = args[\"net\"](num_classes=args[\"classes\"])\n","net.load_state_dict(torch.load(args[\"weight_path\"]))\n","\n","unlearning_teacher = args[\"net\"](num_classes=args[\"classes\"])\n","\n","if args[\"gpu\"]:\n","    net = net.cuda()\n","    unlearning_teacher = unlearning_teacher.cuda()\n","\n","\n","root = \"./data\"\n","\n","\n","img_size = 224 if args[\"net\"] == ViT else 32\n","trainset = datasets_dict[args[\"dataset\"]](\n","    root=root, download=True, train=True, unlearning=True, img_size=img_size\n",")\n","validset = datasets_dict[args[\"dataset\"]](\n","    root=root, download=True, train=False, unlearning=True, img_size=img_size\n",")\n","\n","trainloader = DataLoader(trainset, num_workers=4, batch_size=args[\"b\"], shuffle=True)\n","validloader = DataLoader(validset, num_workers=4, batch_size=args[\"b\"], shuffle=False)\n","\n","forget_train, retain_train = torch.utils.data.random_split(\n","    trainset, [forget_perc, 1 - forget_perc]\n",")\n","forget_train_dl = DataLoader(list(forget_train), batch_size=args[\"b\"])\n","retain_train_dl = DataLoader(list(retain_train), batch_size=args[\"b\"], shuffle=True)\n","forget_valid_dl = forget_train_dl\n","retain_valid_dl = validloader\n","\n","model_size_scaler = args[\"alpha\"]\n","\n","full_train_dl = DataLoader(\n","    ConcatDataset((retain_train_dl.dataset, forget_train_dl.dataset)),\n","    batch_size=batch_size,\n",")\n","\n","kwargs = {\n","    \"model\": net,\n","    \"unlearning_teacher\": unlearning_teacher,\n","    \"retain_train_dl\": retain_train_dl,\n","    \"retain_valid_dl\": retain_valid_dl,\n","    \"forget_train_dl\": forget_train_dl,\n","    \"forget_valid_dl\": forget_valid_dl,\n","    \"full_train_dl\": full_train_dl,\n","    \"valid_dl\": validloader,\n","    \"lr\": args[\"lr\"],\n","    \"batch_size\": batch_size,\n","    \"dampening_constant\": args[\"gamma\"],\n","    \"selection_weighting\": model_size_scaler,\n","    \"num_classes\": args[\"classes\"],\n","    \"dataset_name\": args[\"dataset\"],\n","    \"device\": \"cuda\" if args[\"gpu\"] else \"cpu\",\n","    \"model_name\": args[\"model_name\"],\n","    \"epochs\": args[\"epochs\"]\n","}\n","\n","start = time.time()\n","\n","testacc, retainacc, zrf, mia, d_f = args[\"method\"](\n","    **kwargs\n",")\n","end = time.time()\n","time_elapsed = end - start\n","\n","res_dict = {\n","        \"TestAcc\": testacc,\n","        \"RetainTestAcc\": retainacc,\n","        \"ZRF\": zrf,\n","        \"MIA\": mia,\n","        \"Df\": d_f,\n","        \"MethodTime\": time_elapsed,  \n","    }\n","\n","for k,v in res_dict.items():\n","    print(k + \": \" + str(v))"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4422994,"sourceId":7598284,"sourceType":"datasetVersion"},{"datasetId":4459832,"sourceId":7650355,"sourceType":"datasetVersion"},{"datasetId":4674952,"sourceId":7949664,"sourceType":"datasetVersion"},{"datasetId":4675140,"sourceId":7949916,"sourceType":"datasetVersion"},{"datasetId":4675459,"sourceId":7950384,"sourceType":"datasetVersion"},{"datasetId":4910024,"isSourceIdPinned":true,"sourceId":8270713,"sourceType":"datasetVersion"},{"datasetId":4843393,"sourceId":8180869,"sourceType":"datasetVersion"}],"dockerImageVersionId":30587,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
