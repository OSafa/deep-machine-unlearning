{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8270713,"sourceType":"datasetVersion","datasetId":4910024,"isSourceIdPinned":true}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport sys\nimport argparse\nimport re\nimport time\nimport random\nfrom datetime import datetime\nfrom typing import Any, Tuple, Dict, List\nfrom copy import deepcopy\nimport copy\nimport math\nimport shutil\n\n\nfrom tqdm import tqdm\nfrom sklearn import linear_model, model_selection\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\nimport torch\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom torch.nn import functional as F\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nfrom torch.utils.data import DataLoader, Dataset, Subset, ConcatDataset, dataset\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import CIFAR100, CIFAR10, ImageFolder\nfrom torchvision.models import resnet18\nfrom torch.autograd import Variable\nfrom torchvision import datasets, models, transforms\nimport matplotlib.pyplot as plt\nfrom transformers import ViTModel, ViTFeatureExtractor\nimport seaborn as sns\nimport scipy.stats as stats\nimport pandas as pd\nfrom transformers import AutoTokenizer, AutoModel, Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split\nfrom transformers.data.processors import SingleSentenceClassificationProcessor, InputFeatures\nfrom transformers import AutoModel, AutoTokenizer , AutoModelForSequenceClassification, AutoConfig\n\n\n# Original code from https://github.com/weiaicunzai/pytorch-cifar100 <- refer to this repo for comments","metadata":{"execution":{"iopub.status.busy":"2024-07-04T06:05:35.035546Z","iopub.execute_input":"2024-07-04T06:05:35.035877Z","iopub.status.idle":"2024-07-04T06:05:35.047475Z","shell.execute_reply.started":"2024-07-04T06:05:35.035846Z","shell.execute_reply":"2024-07-04T06:05:35.046487Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"DATE_FORMAT = \"%A_%d_%B_%Y_%Hh_%Mm_%Ss\"\n\n# time of script run\nTIME_NOW = datetime.now().strftime(DATE_FORMAT)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T06:05:35.049028Z","iopub.execute_input":"2024-07-04T06:05:35.049366Z","iopub.status.idle":"2024-07-04T06:05:35.063883Z","shell.execute_reply.started":"2024-07-04T06:05:35.049336Z","shell.execute_reply":"2024-07-04T06:05:35.062948Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"class HARD():\n    def __init__(self, train, file_path):\n        df = pd.read_csv(file_path)\n        if(train):\n            df = df.loc[:40000]\n        else:\n            df = df.loc[40000:]\n        \n        self.data = df[\"review\"].tolist()\n        self.targets = df[\"rating\"].tolist()\n        model_dir = \"/kaggle/input/marbert-hard-data\"\n        \n        tokenizer = AutoTokenizer.from_pretrained(model_dir)\n        \n        dataset = SingleSentenceClassificationProcessor(mode='classification')\n        dataset.add_examples(texts_or_text_and_labels=self.data,overwrite_examples = True)\n        \n        tokenizer.max_len = 512\n        self.data = dataset.get_features(tokenizer = tokenizer, max_length =512)\n    \n            \n    def __getitem__(self, index):\n        review = self.data[index]\n        review_dict = {\"input_ids\":torch.tensor(review.input_ids), \"attention_mask\": torch.tensor(review.attention_mask)}\n        return (review_dict,self.targets[index])\n    \n    def __len__(self):\n        # Assuming 'data' is a list attribute, return its length\n        return len(self.data)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T06:05:35.066492Z","iopub.execute_input":"2024-07-04T06:05:35.066852Z","iopub.status.idle":"2024-07-04T06:05:35.076724Z","shell.execute_reply.started":"2024-07-04T06:05:35.066817Z","shell.execute_reply":"2024-07-04T06:05:35.075890Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"class Marbert(nn.Module):\n    def __init__(self, num_classes=5, model_dir=\"\", **kwargs):\n        super(Marbert, self).__init__()\n       \n        config = AutoConfig.from_pretrained(model_dir, num_labels=num_classes)\n        \n        self.base = AutoModelForSequenceClassification.from_pretrained(model_dir, config = config)\n        self.num_classes = num_classes\n        self.config = config\n    \n    def forward(self, input_ids, attention_mask):\n        # Forward pass computation\n        outputs = self.base(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        return logits\n\n    def __call__(self, reviews):\n        return super(Marbert, self).__call__(reviews['input_ids'].to(torch.int64).to(\"cuda\"), reviews['attention_mask'].to(torch.int64).to(\"cuda\")).to(\"cuda\")","metadata":{"execution":{"iopub.status.busy":"2024-07-04T06:05:35.077893Z","iopub.execute_input":"2024-07-04T06:05:35.078247Z","iopub.status.idle":"2024-07-04T06:05:35.091518Z","shell.execute_reply.started":"2024-07-04T06:05:35.078215Z","shell.execute_reply":"2024-07-04T06:05:35.090591Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"datasets_dict = {\n    \"HARD\": HARD\n}","metadata":{"execution":{"iopub.status.busy":"2024-07-04T06:05:35.093043Z","iopub.execute_input":"2024-07-04T06:05:35.093343Z","iopub.status.idle":"2024-07-04T06:05:35.101548Z","shell.execute_reply.started":"2024-07-04T06:05:35.093319Z","shell.execute_reply":"2024-07-04T06:05:35.100623Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nimport numpy as np\n\ndef accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() / len(preds)) * 100\n\n\ndef training_step(model, batch, device):\n    review, rating, _ = batch\n    out = model(review)  # Generate predictions\n    loss = F.cross_entropy(out, rating)  # Calculate loss\n    return loss\n\n\ndef validation_step(model, batch, device):\n    review, rating = batch\n    rating = rating.to(\"cuda\")\n    out = model(review)  # Generate predictions\n    loss = F.cross_entropy(out, rating)  # Calculate loss\n    acc = accuracy(out, rating)  # Calculate accuracy\n    return {\"Loss\": loss.detach(), \"Acc\": acc}\n\n\ndef validation_epoch_end(model, outputs):\n    batch_losses = [x[\"Loss\"] for x in outputs]\n    epoch_loss = torch.stack(batch_losses).mean()  # Combine losses\n    batch_accs = [x[\"Acc\"] for x in outputs]\n    epoch_acc = torch.stack(batch_accs).mean()  # Combine accuracies\n    return {\"Loss\": epoch_loss.item(), \"Acc\": epoch_acc.item()}\n\n\ndef epoch_end(model, epoch, result):\n    print(\n        \"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch,\n            result[\"lrs\"][-1],\n            result[\"train_loss\"],\n            result[\"Loss\"],\n            result[\"Acc\"],\n        )\n    )\n\n\n@torch.no_grad()\ndef evaluate(model, val_loader, device):\n    model.eval()\n    outputs = [validation_step(model, batch, device) for batch in val_loader]\n    return validation_epoch_end(model, outputs)\n\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group[\"lr\"]\n\ndef fit_one_unlearning_cycle(epochs, model, train_loader, val_loader, lr, device):\n    history = []\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    for epoch in range(epochs):\n        model.train()\n        train_losses = []\n        lrs = []\n        for batch in train_loader:\n            loss = training_step(model, batch, device)\n            loss.backward()\n            train_losses.append(loss.detach().cpu())\n\n            optimizer.step()\n            optimizer.zero_grad()\n\n            lrs.append(get_lr(optimizer))\n\n        result = evaluate(model, val_loader, device)\n        result[\"train_loss\"] = torch.stack(train_losses).mean()\n        result[\"lrs\"] = lrs\n        epoch_end(model, epoch, result)\n        history.append(result)\n    return history\n\nclass UNSIR_noise(torch.nn.Module):\n    def __init__(self, *dim):\n        super().__init__()\n        # marbert tokenizer vocab size is 100000\n        rand_noise = 1 + (100000 - 1) * torch.rand(*dim)\n        self.noise = torch.nn.Parameter(rand_noise, requires_grad=True)\n\n    def forward(self):\n        return self.noise\n\n\ndef UNSIR_noise_train(\n    noise, model, forget_class_label, num_epochs, noise_batch_size, device=\"cuda\"\n):\n    opt = torch.optim.Adam(noise.parameters(), lr=0.1)\n    model.to(device)  # Ensure the model is on the correct device\n    noise.to(device)  # Ensure the noise parameters are on the correct device\n\n    for epoch in range(num_epochs):\n        inputs = noise()\n        inputs = inputs.to(device)\n        attention_mask = torch.ones(inputs.shape).to(device)\n        \n        dict_inputs = {'input_ids': inputs, 'attention_mask': attention_mask}\n        labels = torch.full((noise_batch_size,), forget_class_label, device=device, dtype=torch.long)\n        \n        outputs = model(dict_inputs)\n        loss = -F.cross_entropy(outputs, labels) + 0.1 * torch.mean(\n                torch.sum(inputs**2, dim=1)\n        )\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n\n        total_loss = loss.item()\n        \n        if epoch % 4 == 0:\n            print(f\"Epoch {epoch} Loss: {total_loss}\")\n\n    return noise\n\n\ndef UNSIR_create_noisy_loader(\n    noise,\n    forget_class_label,\n    retain_samples,\n    batch_size,\n    num_noise_batches=80,\n    device=\"cuda\",\n):\n    noisy_data = []\n    for i in range(num_noise_batches):\n        batch = noise()\n        for i in range(batch.size(0)):\n            dict_inputs = {'input_ids': batch[i].detach().cpu(),'attention_mask':torch.ones(batch[i].shape).detach().cpu()}\n            noisy_data.append(\n                (\n                    dict_inputs,\n                    torch.tensor(forget_class_label).to(device),\n                    torch.tensor(forget_class_label).to(device),\n                )\n            )\n    other_samples = []\n    for i in range(len(retain_samples)):\n        dict_inputs = {'input_ids': retain_samples[i][0]['input_ids'].cpu(),\n                        'attention_mask':retain_samples[i][0]['attention_mask'].cpu()}\n        other_samples.append(\n            (\n                dict_inputs,\n                torch.tensor(retain_samples[i][1]).to(device),\n                torch.tensor(retain_samples[i][1]).to(device),\n            )\n        )\n    noisy_data += other_samples\n    noisy_loader = DataLoader(noisy_data, batch_size=batch_size, shuffle=True)\n\n    return noisy_loader","metadata":{"execution":{"iopub.status.busy":"2024-07-04T06:05:35.103091Z","iopub.execute_input":"2024-07-04T06:05:35.103629Z","iopub.status.idle":"2024-07-04T06:05:35.133777Z","shell.execute_reply.started":"2024-07-04T06:05:35.103598Z","shell.execute_reply":"2024-07-04T06:05:35.132852Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nFrom https://github.com/vikram2000b/bad-teaching-unlearning / https://arxiv.org/abs/2205.08096\n\"\"\"\n\ndef JSDiv(p, q):\n    m = (p + q) / 2\n    return 0.5 * F.kl_div(torch.log(p), m) + 0.5 * F.kl_div(torch.log(q), m)\n\n\n# ZRF/UnLearningScore https://arxiv.org/abs/2205.08096\ndef UnLearningScore(tmodel, gold_model, forget_dl, batch_size, device):\n    model_preds = []\n    gold_model_preds = []\n    with torch.no_grad():\n        for batch in forget_dl:\n            x, y = batch\n            x = {'input_ids': x['input_ids'].to(device),\n                    'attention_mask':x['attention_mask'].to(device)}\n            model_output = tmodel(x)\n            gold_model_output = gold_model(x)\n            model_preds.append(F.softmax(model_output, dim=1).detach().cpu())\n            gold_model_preds.append(F.softmax(gold_model_output, dim=1).detach().cpu())\n\n    model_preds = torch.cat(model_preds, axis=0)\n    gold_model_preds = torch.cat(gold_model_preds, axis=0)\n    return 1 - JSDiv(model_preds, gold_model_preds)\n\n\ndef entropy(p, dim=-1, keepdim=False):\n    return -torch.where(p > 0, p * p.log(), p.new([0.0])).sum(dim=dim, keepdim=keepdim)\n\n\ndef collect_prob(data_loader, model):\n    data_loader = torch.utils.data.DataLoader(\n        data_loader.dataset, batch_size=1, shuffle=False\n    )\n    prob = []\n    with torch.no_grad():\n        for batch in data_loader:\n            data,target = batch\n            output = model(data)\n            prob.append(F.softmax(output, dim=-1).data)\n    return torch.cat(prob)\n\n\n# https://arxiv.org/abs/2205.08096\ndef get_membership_attack_data(retain_loader, forget_loader, test_loader, model):\n    retain_prob = collect_prob(retain_loader, model)\n    forget_prob = collect_prob(forget_loader, model)\n    test_prob = collect_prob(test_loader, model)\n\n    X_r = (\n        torch.cat([entropy(retain_prob), entropy(test_prob)])\n        .cpu()\n        .numpy()\n        .reshape(-1, 1)\n    )\n    Y_r = np.concatenate([np.ones(len(retain_prob)), np.zeros(len(test_prob))])\n\n    X_f = entropy(forget_prob).cpu().numpy().reshape(-1, 1)\n    Y_f = np.concatenate([np.ones(len(forget_prob))])\n    return X_f, Y_f, X_r, Y_r\n\n\n# https://arxiv.org/abs/2205.08096\ndef get_membership_attack_prob(retain_loader, forget_loader, test_loader, model):\n    X_f, Y_f, X_r, Y_r = get_membership_attack_data(\n        retain_loader, forget_loader, test_loader, model\n    )\n    # clf = SVC(C=3,gamma='auto',kernel='rbf')\n    clf = LogisticRegression(\n        class_weight=\"balanced\", solver=\"lbfgs\", multi_class=\"multinomial\"\n    )\n    clf.fit(X_r, Y_r)\n    results = clf.predict(X_f)\n    return results.mean()\n\n\n@torch.no_grad()\ndef actv_dist(model1, model2, dataloader, device=\"cuda\"):\n    print(\"actv_dist\")\n    sftmx = nn.Softmax(dim=1)\n    distances = []\n    for batch in dataloader:\n        x, _, _ = batch\n        x = x.to(device)\n        model1_out = model1(x)\n        model2_out = model2(x)\n        diff = torch.sqrt(\n            torch.sum(\n                torch.square(\n                    F.softmax(model1_out, dim=1) - F.softmax(model2_out, dim=1)\n                ),\n                axis=1,\n            )\n        )\n        diff = diff.detach().cpu()\n        distances.append(diff)\n    distances = torch.cat(distances, axis=0)\n    return distances.mean()","metadata":{"execution":{"iopub.status.busy":"2024-07-04T06:05:35.135167Z","iopub.execute_input":"2024-07-04T06:05:35.135433Z","iopub.status.idle":"2024-07-04T06:05:35.157430Z","shell.execute_reply.started":"2024-07-04T06:05:35.135410Z","shell.execute_reply":"2024-07-04T06:05:35.156524Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# Returns metrics\ndef get_metric_scores(\n    model,\n    unlearning_teacher,\n    retain_train_dl,\n    retain_valid_dl,\n    forget_train_dl,\n    forget_valid_dl,\n    valid_dl,\n    device,\n):\n    loss_acc_dict = evaluate(model, valid_dl, device)\n    retain_acc_dict = evaluate(model, retain_valid_dl, device)\n    zrf = UnLearningScore(model, unlearning_teacher, forget_valid_dl, 128, device)\n    d_f = evaluate(model, forget_valid_dl, device)\n    mia = get_membership_attack_prob(retain_train_dl, forget_train_dl, valid_dl, model)\n\n    return (loss_acc_dict[\"Acc\"], retain_acc_dict[\"Acc\"], zrf, mia, d_f[\"Acc\"])\n\n\n# Implementation from https://github.com/vikram2000b/Fast-Machine-Unlearning\ndef UNSIR(\n    model,\n    unlearning_teacher,\n    retain_train_dl,\n    retain_valid_dl,\n    forget_train_dl,\n    forget_valid_dl,\n    valid_dl,\n    num_classes,\n    forget_class,\n    device,\n    **kwargs,\n):\n    \n    classwise_train = get_classwise_ds_marbert(\n        ConcatDataset((retain_train_dl.dataset, forget_train_dl.dataset)), num_classes, \"full\"\n    )\n    noise_batch_size = 8\n    retain_valid_dl = DataLoader(retain_valid_dl.dataset, batch_size=noise_batch_size)\n    # collect some samples from each class\n    num_samples = 500\n    retain_samples = []\n    for i in range(num_classes):\n        if i != forget_class:\n            retain_samples += classwise_train[i][:num_samples]\n\n    forget_class_label = forget_class\n#     img_shape = next(iter(retain_train_dl.dataset))[0].shape[-1]\n    noise = UNSIR_noise(noise_batch_size, 512).to(device)\n    noise = UNSIR_noise_train(\n        noise, model, forget_class_label, 25, noise_batch_size, device=device\n    )\n    noisy_loader = UNSIR_create_noisy_loader(\n        noise,\n        forget_class_label,\n        retain_samples,\n        batch_size=noise_batch_size,\n        device=device,\n    )\n    # impair step\n    _ = fit_one_unlearning_cycle(\n        1, model, noisy_loader, retain_valid_dl, device=device, lr=0.0001\n    )\n    \n    # repair step\n    other_samples = []\n    for i in range(len(retain_samples)):\n        dict_inputs = {'input_ids': retain_samples[i][0]['input_ids'].to(device),\n                    'attention_mask':retain_samples[i][0]['attention_mask'].to(device)}\n        other_samples.append(\n            (\n                dict_inputs,\n                torch.tensor(retain_samples[i][1]).to(device),\n                torch.tensor(retain_samples[i][1]).to(device),\n            )\n        )\n\n    heal_loader = torch.utils.data.DataLoader(\n        other_samples, batch_size=8, shuffle=True\n    )\n    _ = fit_one_unlearning_cycle(\n        1, model, heal_loader, retain_valid_dl, device=device, lr=0.0001\n    )\n    \n    return get_metric_scores(\n        model,\n        unlearning_teacher,\n        retain_train_dl,\n        retain_valid_dl,\n        forget_train_dl,\n        forget_valid_dl,\n        valid_dl,\n        device,\n    )\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-04T06:05:35.158955Z","iopub.execute_input":"2024-07-04T06:05:35.159287Z","iopub.status.idle":"2024-07-04T06:05:35.173775Z","shell.execute_reply.started":"2024-07-04T06:05:35.159257Z","shell.execute_reply":"2024-07-04T06:05:35.172890Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"def get_classwise_ds_marbert(ds, num_classes, forget_type):\n    classwise_ds = {}\n    for i in range(num_classes):\n        classwise_ds[i] = []\n\n    for review,rating in ds:\n        classwise_ds[rating].append((review,rating))\n    return classwise_ds\n\n# Creates datasets for method execution\ndef build_retain_forget_sets_marbert(\n    classwise_train, classwise_test, num_classes, forget_class\n):\n    # Getting the forget and retain validation data\n    forget_valid = []\n    for cls in range(num_classes):\n        if cls == forget_class:\n            for review,rating in classwise_test[cls]:\n                forget_valid.append((review,rating))\n\n    retain_valid = []\n    for cls in range(num_classes):\n        if cls != forget_class:\n            for review,rating in classwise_test[cls]:\n                retain_valid.append((review,rating))\n\n    forget_train = []\n    for cls in range(num_classes):\n        if cls == forget_class:\n            for review,rating in classwise_train[cls]:\n                forget_train.append((review,rating))\n\n    retain_train = []\n    for cls in range(num_classes):\n        if cls != forget_class:\n            for review,rating in classwise_train[cls]:\n                retain_train.append((review,rating))\n    return (retain_train, retain_valid, forget_train, forget_valid)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T06:05:35.176004Z","iopub.execute_input":"2024-07-04T06:05:35.176296Z","iopub.status.idle":"2024-07-04T06:05:35.188326Z","shell.execute_reply.started":"2024-07-04T06:05:35.176272Z","shell.execute_reply":"2024-07-04T06:05:35.187531Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"# Full Class","metadata":{}},{"cell_type":"code","source":"for f_class in [1, 2, 3, 4, 5]:\n    print(str(f_class) + \" :-\")\n\n    \"\"\"\n    This file is used to collect all arguments for the experiment, prepare the dataloaders, call the method for forgetting, and gather/log the metrics.\n    Methods are executed in the strategies file.\n    \"\"\"\n    res = []\n    \"\"\"\n    Get Args\n    \"\"\"\n    args = {\n        \"net\": Marbert,\n        \"weight_path\": \"/kaggle/working/ResNet18-Cifar100-197-best.pth\",\n        #choices=[\"Cifar10\", \"Cifar20\", \"Cifar100\", \"PinsFaceRecognition\", \"HARD\"]\n        \"dataset\": \"HARD\",\n    \"classes\": 5,\n    \"gpu\": True,\n    \"b\": 32,\n    \"warm\": 1,\n    \"lr\": 0.1,\n    \"method\": UNSIR,\n    \"forget_class\": f_class,\n    \"epochs\": 1,\n    \"seed\": 0\n    }\n\n    # Set seeds\n    torch.manual_seed(args[\"seed\"])\n    np.random.seed(args[\"seed\"])\n    random.seed(args[\"seed\"])\n\n\n    forget_class = args[\"forget_class\"]-1\n\n    batch_size = args[\"b\"]\n\n\n    # get network\n    net = args[\"net\"](num_classes=args[\"classes\"],model_dir=\"/kaggle/input/marbert-hard-data\")\n\n    # for bad teacher\n    unlearning_teacher = args[\"net\"](num_classes=args[\"classes\"],model_dir = \"/kaggle/input/marbert-hard-data\")\n\n    if args[\"gpu\"]:\n        net = net.cuda()\n        unlearning_teacher = unlearning_teacher.cuda()\n\n    trainset = datasets_dict[args[\"dataset\"]](\n        train=True, file_path=\"/kaggle/input/marbert-hard-data/HARD_50000_clean_shifted.csv\"\n    )\n    validset = datasets_dict[args[\"dataset\"]](\n        train=False, file_path=\"/kaggle/input/marbert-hard-data/HARD_50000_clean_shifted.csv\"\n    )\n\n    # Set up the dataloaders and prepare the datasets\n    trainloader = DataLoader(trainset, num_workers=4, batch_size=args[\"b\"], shuffle=True)\n    validloader = DataLoader(validset, num_workers=4, batch_size=args[\"b\"], shuffle=False)\n\n    classwise_train, classwise_test = get_classwise_ds_marbert(\n        trainset, args[\"classes\"],\"full\"\n    ), get_classwise_ds_marbert(validset, args[\"classes\"],\"full\")\n\n    (\n        retain_train,\n        retain_valid,\n        forget_train,\n        forget_valid,\n    ) = build_retain_forget_sets_marbert(\n        classwise_train, classwise_test, args[\"classes\"], forget_class\n    )\n    forget_valid_dl = DataLoader(forget_valid, batch_size)\n    retain_valid_dl = DataLoader(retain_valid, batch_size)\n\n    forget_train_dl = DataLoader(forget_train, batch_size)\n    retain_train_dl = DataLoader(retain_train, batch_size, shuffle=True)\n    full_train_dl = DataLoader(\n        ConcatDataset((retain_train_dl.dataset, forget_train_dl.dataset)),\n        batch_size=batch_size,\n    )\n\n    kwargs = {\n        \"model\": net,\n        \"unlearning_teacher\": unlearning_teacher,\n        \"retain_train_dl\": retain_train_dl,\n        \"retain_valid_dl\": retain_valid_dl,\n        \"forget_train_dl\": forget_train_dl,\n        \"forget_valid_dl\": forget_valid_dl,\n        \"full_train_dl\": full_train_dl,\n        \"valid_dl\": validloader,\n        \"forget_class\": forget_class,\n        \"num_classes\": args[\"classes\"],\n        \"dataset_name\": args[\"dataset\"],\n        \"device\": \"cuda\" if args[\"gpu\"] else \"cpu\",\n        \"model_name\": args[\"net\"],\n    }\n\n    start = time.time()\n\n    # executes the method passed via args\n    testacc, retainacc, zrf, mia, d_f = args[\"method\"](\n        **kwargs\n    )\n    end = time.time()\n    time_elapsed = end - start\n\n    # Logging\n    res_dict = {\n            \"TestAcc\": testacc,\n            \"RetainTestAcc\": retainacc,\n            \"Df\": d_f,\n            \"ZRF\": zrf,\n            \"MIA\": mia,\n            \"model_scaler\": model_size_scaler,\n            \"MethodTime\": time_elapsed,  # do not forget to deduct baseline time from it to remove results calc (acc, MIA, ...)\n        }\n\n    for k,v in res_dict.items():\n        print(k + \": \" + str(v))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}